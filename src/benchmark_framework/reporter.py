"""Markdown report generator for benchmark results."""

from __future__ import annotations

from collections import defaultdict
from datetime import datetime, timezone
from pathlib import Path

from .models import MetricDefinition, RunDefinition, RunResult
from .registry import Registry


class BenchmarkReporter:
    """Generates Markdown comparison reports from benchmark results.

    Reads results.json and produces a structured Markdown document with
    comparison tables highlighting the best iteration per metric.

    Args:
        config_dir: Path to the directory containing JSON config files.
    """

    def __init__(self, config_dir: str | Path) -> None:
        self.config_dir = Path(config_dir)
        self.iterations_config = Registry.load_iterations(
            self.config_dir / "iterations.json"
        )
        self.metrics_config = Registry.load_metrics(
            self.config_dir / "metrics.json"
        )
        self.runs_config = Registry.load_runs(
            self.config_dir / "runs.json"
        )
        self.results_config = Registry.load_results(
            self.config_dir / "results.json"
        )

    def generate_report(self, output_path: str | Path | None = None) -> str:
        """Generate a Markdown comparison report.

        Args:
            output_path: Optional path to write the report file.
                If None, only returns the report string.

        Returns:
            The full Markdown report as a string.
        """
        sections: list[str] = []
        sections.append(self._header())

        completed_runs = [
            r for r in self.runs_config.runs if r.status == "completed"
        ]

        if not completed_runs:
            sections.append("No completed runs to report.\n")
        else:
            for run_def in completed_runs:
                sections.append(self._run_section(run_def))

        sections.append(self._footer())

        report = "\n".join(sections)

        if output_path is not None:
            Path(output_path).write_text(report, encoding="utf-8")

        return report

    def _header(self) -> str:
        """Generate report header."""
        project = self.iterations_config.project or "Benchmark"
        now = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M UTC")
        return (
            f"# {project} — Benchmark Comparison Report\n\n"
            f"Generated: {now}\n"
        )

    def _footer(self) -> str:
        """Generate report footer."""
        return (
            "\n---\n\n"
            "*Report generated by "
            "[benchmark-framework](https://github.com/jeanremacle/benchmark-framework)*\n"
        )

    def _run_section(self, run_def: RunDefinition) -> str:
        """Generate a section for a single run."""
        run_results = [
            r for r in self.results_config.results
            if r.run_id == run_def.id
        ]

        if not run_results:
            return f"## {run_def.name}\n\nNo results recorded.\n"

        lines: list[str] = []
        lines.append(f"## {run_def.name}\n")
        if run_def.description:
            lines.append(f"{run_def.description}\n")

        # Build comparison table
        lines.append(self._comparison_table(run_def, run_results))

        # Add narrative
        lines.append(self._narrative(run_def, run_results))

        return "\n".join(lines)

    def _comparison_table(
        self,
        run_def: RunDefinition,
        run_results: list[RunResult],
    ) -> str:
        """Generate a Markdown comparison table."""
        metric_map = {m.id: m for m in self.metrics_config.metrics}
        iteration_map = {
            it.id: it for it in self.iterations_config.iterations
        }

        # Group results: metric_id -> iteration_id -> value
        by_metric: dict[str, dict[str, float]] = defaultdict(dict)
        for r in run_results:
            by_metric[r.metric_id][r.iteration_id] = r.value

        # Determine column order from run definition
        iter_ids = run_def.iteration_ids
        metric_ids = run_def.metric_ids

        # Header row
        iter_names = [
            iteration_map[iid].name if iid in iteration_map else iid
            for iid in iter_ids
        ]
        header = "| Metric | " + " | ".join(iter_names) + " |"
        separator = "|" + "|".join(
            ["---"] * (len(iter_ids) + 1)
        ) + "|"

        rows: list[str] = [header, separator]

        for mid in metric_ids:
            metric_def = metric_map.get(mid)
            metric_name = metric_def.name if metric_def else mid
            values = by_metric.get(mid, {})

            best_id = self._find_best(values, metric_def)

            cells: list[str] = []
            for iid in iter_ids:
                val = values.get(iid)
                if val is None:
                    cells.append("—")
                else:
                    formatted = self._format_value(val, metric_def)
                    if iid == best_id and len(values) > 1:
                        formatted = f"**{formatted}**"
                    cells.append(formatted)

            row = f"| {metric_name} | " + " | ".join(cells) + " |"
            rows.append(row)

        return "\n".join(rows) + "\n"

    def _narrative(
        self,
        run_def: RunDefinition,
        run_results: list[RunResult],
    ) -> str:
        """Generate narrative analysis of results."""
        metric_map = {m.id: m for m in self.metrics_config.metrics}
        iteration_map = {
            it.id: it for it in self.iterations_config.iterations
        }

        by_metric: dict[str, dict[str, float]] = defaultdict(dict)
        for r in run_results:
            by_metric[r.metric_id][r.iteration_id] = r.value

        lines: list[str] = ["### Analysis\n"]

        for mid in run_def.metric_ids:
            metric_def = metric_map.get(mid)
            metric_name = metric_def.name if metric_def else mid
            values = by_metric.get(mid, {})

            if not values:
                continue

            best_id = self._find_best(values, metric_def)
            if best_id is None:
                continue

            best_name = (
                iteration_map[best_id].name
                if best_id in iteration_map
                else best_id
            )
            best_val = values[best_id]
            direction = "highest" if (
                metric_def and metric_def.higher_is_better
            ) else "lowest"

            lines.append(
                f"- **{metric_name}**: {best_name} achieves the "
                f"{direction} value ({self._format_value(best_val, metric_def)}"
                f"{' ' + metric_def.unit if metric_def and metric_def.unit else ''})"
            )

            # Show comparison if multiple iterations
            if len(values) > 1:
                sorted_items = sorted(
                    values.items(),
                    key=lambda x: x[1],
                    reverse=bool(
                        metric_def and metric_def.higher_is_better
                    ),
                )
                if len(sorted_items) >= 2:
                    worst_id = sorted_items[-1][0]
                    worst_val = sorted_items[-1][1]
                    if best_val != 0:
                        diff_pct = abs(
                            (worst_val - best_val) / best_val * 100
                        )
                        worst_name = (
                            iteration_map[worst_id].name
                            if worst_id in iteration_map
                            else worst_id
                        )
                        lines.append(
                            f"  - {worst_name} differs by "
                            f"{diff_pct:.1f}%"
                        )

        lines.append("")
        return "\n".join(lines)

    @staticmethod
    def _find_best(
        values: dict[str, float],
        metric_def: MetricDefinition | None,
    ) -> str | None:
        """Find the best iteration ID for a given metric."""
        if not values:
            return None
        if metric_def and metric_def.higher_is_better:
            return max(values, key=lambda k: values[k])
        return min(values, key=lambda k: values[k])

    @staticmethod
    def _format_value(
        value: float, metric_def: MetricDefinition | None
    ) -> str:
        """Format a metric value for display."""
        if value == int(value):
            return str(int(value))
        return f"{value:.4f}"
